# this must be provided
#run_name: 'copy str'
#run_description: "use transformer to copy str"

# here is data, model and metric, it depends on the project
# y_rgb have no effect on these
# ==============================
data:
  max_len: 50
  vocab_n: 33
  data_n: 320
  batch_size: 16
model:
  type: "TransformerForConditionalLLM"
  vocab_n: 33
  num_layers: 2
  d: 512
  n: 8
  max_len: 50
  d_ff: 2048
  dropout: 0.1
  use_rope: true
metric:
  # 自定义metric可以按照model保存在这

# ==============================
#optimizer:
  # 使用的是AdamW
#  lr: 3e-4
#  weight_decay: 0.01
scheduler:
  type: "LinearWarmupCosineLRScheduler"
  min_lr: 3e-5
  max_lr: 3e-4
  warmup_rate: 0.05
  warmup_start_lr: 1e-5
training:
  epochs: 10
#  is_sweep: false
#  ds_config: null
#  gradient_accumulation: 1
#  resume_from: null
#  activation_checkpoint: [
#    'transformer.encoder.layers.0',
#    'transformer.encoder.layers.1',
#    'transformer.decoder.layers.0',
#    'transformer.decoder.layers.1'
#  ]
#  grad_clip: 1.0
#  fp16: false
#
#  print_model: true
#  # false means loss is <not> [greater is better].
#  progress_show:
#    loss: false
#    valid_acc: true
#    test_acc: true
#  valid_every_n_epochs: 1
#  test_every_n_epochs: 1
#  progress_every_n_epochs: 1
#  progress_every_n_batches: 1
#
## below list all the config of runner with the default value
## you can change it if you want
#log:
#  to_file: true
#  folder: "./logs"
#  log_level: "INFO"
#pt:
#  pt_save: true
#  pt_save_dir: "./checkpoints"
#  # 不互斥，若同时指定n_epochs, n_batches, 则都会生效
#  pt_best_monitor:
#    loss: false
#  pt_topk: 3
#  pt_save_n_epochs: 1
#  pt_save_n_batches: null
#wandb:
#  wandb_enable: true
#  wandb_project_name: "y_rgb_example"
#  wandb_offline: false
#  wandb_dir: "./"
#  wandb_tags: ["y_rgb", "example"]
